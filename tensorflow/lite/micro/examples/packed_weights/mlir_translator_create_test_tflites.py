# To add a new cell, type '# %%'
# To add a new markdown cell, type '# %% [markdown]'

import os
import sys
import shutil
import tensorflow as tf
# Numpy is a math library
import numpy as np
# Matplotlib is a graphing library
import matplotlib.pyplot as plt
# math is Python's math library
import math
# We'll use Keras to create a simple model architecture
from tensorflow.keras import layers
import utils
from tensorflow.keras.layers import Dense
from kquant.layers import QuantDense
from kquant.quantizer import TFMinMaxQuantizer, TFEmaMinMaxQuantizer
from kquant.tflite_utils.conversion import translate_from_keras_model, write_C_array_src_and_hdr
from kquant.tflite_utils.interpreter import FlatBufModel
from kquant.tflite_utils.quantization import uniform_quant_params, uniform_encode

# %% [markdown]
# ## Generate data
# Deep learning networks learn to model patterns in underlying data. In this notebook, we're going to train a network to model data generated by a [sine](https://en.wikipedia.org/wiki/Sine) function. This will result in a model that can take a value, `x`, and predict its sine, `y`.
# 
# In a real world application, if you needed the sine of `x`, you could just calculate it directly. However, by training a model to do this, we can demonstrate the basic principles of machine learning.
# 
# In the [hello_world](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro/examples/hello_world) sample for [TensorFlow Lite for Microcontrollers](https://www.tensorflow.org/lite/microcontrollers/overview), we'll use this model to control LEDs that light up in a sequence.
# 
# The code in the following cell will generate a set of random `x` values, calculate their sine values, and display them on a graph:

# %%
# We'll generate this many sample datapoints
SAMPLES = 1000
reload = True
translator = "/home/as/nobackup/aiml_deployment/tensorflow/bazel-bin/tensorflow/compiler/mlir/lite/tf_tfl_translate"

# Set a "seed" value, so we get the same random numbers each time we run this
# notebook.  At least for TF2.1 this choice lproduces nicely convergent learning
# the example model is actually rather unstable with learning failing to converge...

np.random.seed(1)
tf.random.set_seed(1)

# Generate a uniformly distributed set of random numbers in the range from
# 0 to 2Ï€, which covers a complete sine wave oscillation
x_values = np.random.uniform(low=0, high=2*math.pi, size=SAMPLES)

# Shuffle the values to guarantee they're not in order
np.random.shuffle(x_values)

# Calculate the corresponding sine values
y_values = np.sin(x_values)


# %% [markdown]
# ## Add some noise
# Since it was generated directly by the sine function, our data fits a nice, smooth curve.
# 
# However, machine learning models are good at extracting underlying meaning from messy, real world data. To demonstrate this, we can add some noise to our data to approximate something more life-like.
# 
# In the following cell, we'll add some random noise to each value, then draw a new graph:

# %%
# Add a small random number to each y value
y_values += 0.1 * np.random.randn(*y_values.shape)


# %% [markdown]
# ## Split our data
# We now have a noisy dataset that approximates real world data. We'll be using this to train our model.
# 
# To evaluate the accuracy of the model we train, we'll need to compare its predictions to real data and check how well they match up. This evaluation happens during training (where it is referred to as validation) and after training (referred to as testing) It's important in both cases that we use fresh data that was not already used to train the model.
# 
# To ensure we have data to use for evaluation, we'll set some aside before we begin training. We'll reserve 20% of our data for validation, and another 20% for testing. The remaining 60% will be used to train the model. This is a typical split used when training models.
# 
# The following code will split our data and then plot each set as a different color:
# 

# %%
# We'll use 60% of our data for training and 20% for testing. The remaining 20%
# will be used for validation. Calculate the indices of each section.
TRAIN_SPLIT =  int(0.6 * SAMPLES)
TEST_SPLIT = int(0.2 * SAMPLES + TRAIN_SPLIT)

# Use np.split to chop our data into three parts.
# The second argument to np.split is an array of indices where the data will be
# split. We provide two indices, so the data will be divided into three chunks.
x_train, x_test, x_validate = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])
y_train, y_test, y_validate = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])

# Double check that our splits add up correctly
assert (x_train.size + x_validate.size + x_test.size) ==  SAMPLES


def weight_q(name, **kwargs):
    return TFEmaMinMaxQuantizer(
        name=name, init_min=-1.0, init_max=1.0, quant_delay=100,  num_bits=8, decay=0.99, **kwargs
    )

def weight_q_sub8( num_bits ):
    def qtizer(name, **kwargs):
        return TFEmaMinMaxQuantizer(
            name=name, init_min=-1.0, init_max=1.0, quant_delay=100,  num_bits=num_bits, decay=0.99, **kwargs
        )
    return qtizer


def act_q(name, **kwargs):
    return TFEmaMinMaxQuantizer(
        name=name, init_min=-4.0, init_max=4.0, quant_delay=100, num_bits=8, decay=0.99, **kwargs
    )



def quant_hello_world_model(qbits):
    model = tf.keras.Sequential()

    #model.add(layers.Lambda( lambda x: tf.identity(x), name="tap_input", input_shape=(1,)))
    # First layer takes a scalar input and feeds it through 16 "neurons". The
    # neurons decide whether to activate based on the 'relu' activation function.

    model.add(QuantDense(16, activation='relu',
                             input_shape=(1,),
                             kernel_quantizer=weight_q,
                             output_quantizer=act_q,))

    # The new second layer may help the network learn more complex representations
    # Deliberate error with have an input_quantizer despite the previous layer
    # habing and output quantizer...
    model.add(QuantDense(16, activation='relu',
                             kernel_quantizer=weight_q,
                             output_quantizer=act_q))


    # Final layer is a single neuron, since we want to output a single value
    #model.add(QuantDense(1, kernel_quantizer=weight_q, output_quantizer=None))
    # Deliberate error: we don disalbe the output quantizer despite setting an
    # explicit quantization layer for the known-in-advance output range.
    model.add(QuantDense(1, kernel_quantizer=weight_q_sub8(qbits), output_quantizer=None))

    # We need an output fq layer to get final quantization for converter
    model.add(TFMinMaxQuantizer(name="output",
                                qmin=-1.0, qmax=127 / 128.0, quant_delay=100, num_bits=8,
                                narrow_range=False))
    return model

q_params = {}
f_weights = {}
weight_codes = {}

QBITS = [4,5,6,8]
for qbits in QBITS:
    # %%
    tf.keras.backend.clear_session()
    sess = tf.compat.v1.keras.backend.get_session()


    model_q = quant_hello_world_model(qbits)
    #model_ref = hello_world_model()


    BATCH_SIZE = 16
    #model_ref.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
    model_q.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])


    # %%
    # And train....
    logsq = os.path.join(f"logs_{qbits}")
    shutil.rmtree(logsq, ignore_errors=True)
    os.makedirs(logsq, exist_ok=True)
    tensorboard_q = tf.keras.callbacks.TensorBoard(log_dir=logsq)

    weights_save_path = os.path.join("saves", f"mlir_translator_test_{qbits}")
    if reload and os.path.exists(weights_save_path+".index"):
        model_q.load_weights(weights_save_path)
    else:
        history_q = model_q.fit(x_train, y_train, epochs=20, batch_size=BATCH_SIZE,
                                validation_data=(x_validate, y_validate),
                                callbacks=[tensorboard_q])
        model_q.save_weights(weights_save_path)

    #for l in model_q.layers:
    #    if hasattr(l,'output_quantizer'):
    #        print( l.kernel_quantizer.name, l.kernel_quantizer.max_ema.get_weights())
    #    if hasattr(l, "max_ema"):
    #        print(l.max_ema.name, l.max_ema.get_weights())


    #qscale, qzero = uniform_quant_params(-1, 127/128.0, bits=8, narrow_range=False)
    #max_out = model_q.layers[-1].max_ema.non_trainable_weights[0]
    #min_out = model_q.layers[-1].min_ema.non_trainable_weights[0]

    # %% [markdown]
    # ## Evaluate our new model
    # Each training epoch, the model prints out its loss and mean absolute error for training and validation. You can read this in the output above (note that your exact numbers may differ):
    #
    # ```
    # Epoch 600/600
    # 600/600 [==============================] - 0s 109us/sample - loss: 0.0124 - mae: 0.0892 - val_loss: 0.0116 - val_mae: 0.0845
    # ```
    #
    # The following cell will print the same graphs showing our new training history:
    # You can see that we've already got useavble results - validation loss his 0.015, and validation MAE is  0.1.
    #

    # %%


    opt_raw_graph = utils.conversion_raw_graph_def(model_q)

    #pb_pname = "hello_world_raw.pb"
    #pb_dir, pb_fname = os.path.split(pb_pname)
    #tf.io.write_graph( opt_raw_graph, pb_dir, pb_fname, as_text=True)


    # %%
    #tf.keras.backend.clear_session()
    #sess = tf.compat.v1.keras.backend.get_session()
    #model_ref = tf.keras.models.load_model('hello_world.h5')

    # Calculate and print the loss on our test dataset
    loss_q = model_q.evaluate(x_test, y_test)
    pred_y = model_q.predict(x_test)



    # %% [markdown]
    # The evaluation metrics we printed show that the model has a low loss and MAE on the test data, and the predictions line up visually with our data fairly well even with quantized weights.
    #
    # The model isn't perfect; its predictions don't form a smooth sine curve. For instance, the line is almost straight when `x` is between 4.2 and 5.2. If we wanted to go further, we could try further increasing the capacity of the model, perhaps using some techniques to defend from overfitting.
    #
    #
    # ## Convert to TensorFlow Lite
    # We now have an acceptably accurate model in-memory. However, to use this with TensorFlow Lite for Microcontrollers, we'll need to convert it into the correct format and download it as a file. To do this, we'll use the [TensorFlow Lite Converter](https://www.tensorflow.org/lite/convert). The converter outputs a file in a special, space-efficient format for use on memory-constrained devices.
    #
    # * Save the trained model using TF mechansims.
    # * Build a second copy of the quantized model (without training machinery)
    # * Load variables from the saved model
    # * ... and then finally freeze the result.  Ouch!
    # #



    tocolog = os.path.join(logsq, "toco_log")
    shutil.rmtree(tocolog, ignore_errors=True)
    os.makedirs(tocolog, exist_ok=True)

    if qbits < 8:
        type_suff = "_packed"
    else:
        type_suff = ""
    mlir_fname_root = f"hello_world{type_suff}_{qbits}"
    cmdline_args = {'print-ir-after-all':None}
    translate_from_keras_model(model_q, mlir_fname_root, 0.0, math.pi*2, use_toco=False,
                               constant_folding=False, translator=translator,
                               tool_cmdline_args={})
    #translate_from_keras_model(model_q, "hello_world_toco", 0.0, math.pi*2, use_toco=True,
    #                           constant_folding=False)

    write_C_array_src_and_hdr(f"{qbits}-Bit weight 'hello world' sine model", ".", mlir_fname_root,
                              aligned=True,
                              src_datafile=mlir_fname_root+".tflite")

    layer_vars = model_q.layers[-2].get_weights()
    f_weights[qbits] = [v for v in layer_vars[0].flatten()]
    fq_min = layer_vars[3]
    fq_max = layer_vars[4]

    q_params[qbits] = uniform_quant_params(fq_min, fq_max, bits=qbits)

    weight_codes[qbits] = uniform_encode(f_weights[qbits], scale_zero=q_params[qbits], bits=qbits)
    with open(mlir_fname_root+ "refdata.h", "w") as res:

        res.write(f"float {mlir_fname_root}_refdata[][3] = {{\n")
        for x in [ 0.0, 1.0, 3.0, 5.0 ]:
            y = model_q.predict([x])[0][0]
            res.write( f"    {{{x}, {math.sin(x)}, {y}}},\n")
            print( "X", x, "REF", math.sin(x), "MODEL", y)
        res.write("};")


for qbits in QBITS:

    print("QUANT BITWIDTH:", qbits)
    print("WEIGHT PARAMS :", q_params[qbits])
    print("WEIGHT VALUES :", f_weights[qbits])
    print("WEIGHT CODES  :", [hex(w) for w in weight_codes[qbits]])


sys.exit(0)

for qbits in QBITS:
    # TODO Currently don't have a native tflite implmentation
    # of packed weights model(s)... so need to disable for now

    # %% [markdown]
    # ## Test the converted models
    # To prove these models are still accurate after conversion and quantization, we'll use both of them to make predictions and compare these against our test results:

    # %%
    # Instantiate an interpreter for each model
    #sine_model = tf.lite.Interpreter('hello_world_ref.tflite')



    sine_model_mlir =FlatBufModel(mlir_fname_root + '.tflite')
    sine_model_toco = FlatBufModel('hello_world_toco.tflite')

    print("MLIR", sine_model_mlir.out_quantization)
    print("TOCO", sine_model_toco.out_quantization)

    assert sine_model_mlir.out_quantization == sine_model_toco.out_quantization

    qscale = 1.0/sine_model_mlir.out_quantization[0]
    # Create arrays to store the results
    # sine_model_predictions = np.empty(x_test.size)
    sine_model_quantized_predictions = np.empty(x_test.size)

    err_sum = 0
    mismatch = 0
    # Run each model's interpreter for each value and store the results in arrays
    for i in range(x_test.size):
      #sine_model_input().fill(x_test[i])
      #sine_model.invoke()
      #sine_model_predictions[i] = sine_model_output()[0]

      x = x_test[i]
      shape_2d = (1,) + (x.shape)
      shaped_val_x = x.reshape(shape_2d)

      ref_pred = model_q.predict(shaped_val_x).item()
      tfl_pred = sine_model_mlir.predict(shaped_val_x)[0][0]
      sine_model_quantized_predictions[i] = tfl_pred

      err = abs(ref_pred-tfl_pred)/qscale
      if err>4:
           print("TFL ROUNDING DIFF @", i, "TFL", tfl_pred/qscale, "TF", ref_pred/qscale)
           mismatch += 1
      err_sum += err
      i = i*50

    mean_err = err_sum/x_test.size
    print("TFL/TF MISMATCHES", mismatch, "MEAN ERR", mean_err)
    assert mismatch < 8, "Should not have many TFL/TF mismatches"
    assert mean_err < 2, "Should not have large mean error"
